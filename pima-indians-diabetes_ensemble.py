# -*- coding: utf-8 -*-
"""Hyeri_Kim_Ensemble.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H_ahF6ttApaJC-QaJ5ybylsVIjx_i0st

# Ensemble Assignment

#### Author : Hyeri Kim
#### Stu ID : 301208760
#### Course : COMP247
#### Section : 003
#### Professor : Merlin James Rukshan Dennis

### Exercise **1**
"""

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# Task 1: Load the data into a pandas dataframe named df_firstname
df_hyeri = pd.read_csv('pima-indians-diabetes.csv')

# Task 2: Add the column names
column_names = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',
                'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']
df_hyeri.columns = column_names

# Task 3: Initial investigations
# a. Check the names and types of columns
print(df_hyeri.dtypes,"\n")

# b. Check the missing values
print(df_hyeri.isnull().sum(),"\n")

# c. Check the statistics of the numeric fields
print(df_hyeri.describe(),"\n")

# d. Check the categorical values, if any (none in this dataset)

# e. Print out the total number of instances in each class
print(df_hyeri['Outcome'].value_counts(),"\n")

# Task 4: Prepare a standard scaler transformer
transformer_hyeri = StandardScaler()

# Task 5: Split the features from the class
X = df_hyeri.iloc[:, :-1]
y = df_hyeri.iloc[:, -1]

# Task 6: Split the data into train and test sets
X_train_hyeri, X_test_hyeri, y_train_hyeri, y_test_hyeri = train_test_split(X, y, test_size=0.3, random_state=60)

# Task 7: Apply the transformer to the features
X_train_hyeri = transformer_hyeri.fit_transform(X_train_hyeri)
X_test_hyeri = transformer_hyeri.transform(X_test_hyeri)

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, VotingClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier

# Task 8: Define 5 classifiers
logreg_X = LogisticRegression(max_iter=1400)
rfc_X = RandomForestClassifier()
svc_X = SVC()
dtc_X = DecisionTreeClassifier(criterion="entropy", max_depth=42)
etc_X = ExtraTreesClassifier()

# Task 9: Define a voting classifier
voting_clf = VotingClassifier(estimators=[('logreg', logreg_X), ('rfc', rfc_X), ('svc', svc_X),
                                          ('dtc', dtc_X), ('etc', etc_X)], voting='hard')

# create an instance of LogisticRegression
logreg_X = LogisticRegression()

# fit the model on your training data
logreg_X.fit(X_train_hyeri, y_train_hyeri)
rfc_X.fit(X_train_hyeri, y_train_hyeri)
svc_X.fit(X_train_hyeri, y_train_hyeri)
dtc_X.fit(X_train_hyeri, y_train_hyeri)
etc_X.fit(X_train_hyeri, y_train_hyeri)

# Task 10: Fit the training data and predict the first three instances of test data
voting_clf.fit(X_train_hyeri, y_train_hyeri)
y_pred_voting = voting_clf.predict(X_test_hyeri[:3])
y_pred_logreg = logreg_X.predict(X_test_hyeri[:3])
y_pred_rfc = rfc_X.predict(X_test_hyeri[:3])
y_pred_svc = svc_X.predict(X_test_hyeri[:3])
y_pred_dtc = dtc_X.predict(X_test_hyeri[:3])
y_pred_etc = etc_X.predict(X_test_hyeri[:3])

# Task 11: Print out the predicted and actual values for each classifier and instance
print("Predictions and actual values for the voting classifier:")
for i in range(3):
    print("Instance", i+1, "- Predicted:", y_pred_voting[i], "Actual:", y_test_hyeri.iloc[i])

print("\nPredictions and actual values for the logistic regression classifier:")
for i in range(3):
    print("Instance", i+1, "- Predicted:", y_pred_logreg[i], "Actual:", y_test_hyeri.iloc[i])

print("\nPredictions and actual values for the random forest classifier:")
for i in range(3):
    print("Instance", i+1, "- Predicted:", y_pred_rfc[i], "Actual:", y_test_hyeri.iloc[i])

print("\nPredictions and actual values for the support vector machine classifier:")
for i in range(3):
    print("Instance", i+1, "- Predicted:", y_pred_svc[i], "Actual:", y_test_hyeri.iloc[i])

print("\nPredictions and actual values for the decision tree classifier:")
for i in range(3):
    print("Instance", i+1, "- Predicted:", y_pred_dtc[i], "Actual:", y_test_hyeri.iloc[i])

print("\nPredictions and actual values for the extra trees classifier:")
for i in range(3):
    print("Instance", i+1, "- Predicted:", y_pred_etc[i], "Actual:", y_test_hyeri.iloc[i])

# Step 8
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, VotingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC

log_reg_s = LogisticRegression(max_iter=1400)
rand_forest_s = RandomForestClassifier()
svm_s = SVC(probability=True)
dec_tree_s = DecisionTreeClassifier(criterion="entropy", max_depth=42)
extra_trees_s = ExtraTreesClassifier()

# Step 9
voting_clf_soft = VotingClassifier(estimators=[('lr', log_reg_s), ('rf', rand_forest_s), ('svm', svm_s), ('dt', dec_tree_s), ('et', extra_trees_s)], voting='soft')

log_reg_s.fit(X_train_hyeri, y_train_hyeri)
rand_forest_s.fit(X_train_hyeri, y_train_hyeri)
svm_s.fit(X_train_hyeri, y_train_hyeri)
dec_tree_s.fit(X_train_hyeri, y_train_hyeri)
extra_trees_s.fit(X_train_hyeri, y_train_hyeri)
voting_clf_soft.fit(X_train_hyeri, y_train_hyeri)

# Step 10

# Convert the NumPy array to a pandas DataFrame
X_test_hyeri_df = pd.DataFrame(X_test_hyeri)

voting_clf_soft.fit(X_train_hyeri, y_train_hyeri)
# Make predictions using the voting classifier with soft voting
y_pred_soft = voting_clf_soft.predict(X_test_hyeri_df.iloc[:3])

for clf in (log_reg_s, rand_forest_s, svm_s, dec_tree_s, extra_trees_s, voting_clf_soft):
    print(clf.__class__.__name__)
    print("="*30)
    for i in range(3):
        print(f"Instance {i+1}: Predicted={clf.predict(X_test_hyeri_df.iloc[[i]])}, Actual={y_test_hyeri.iloc[i]}")
    print("="*30)

from sklearn.pipeline import Pipeline
from sklearn.model_selection import cross_val_score, cross_val_predict
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.tree import DecisionTreeClassifier

# Step 14
logreg_X = LogisticRegression(max_iter=1400)
rfc_X = RandomForestClassifier()
svm_X = SVC()
dtc_X = DecisionTreeClassifier(criterion="entropy", max_depth=42)
etc_X = ExtraTreesClassifier()

# Step 15
voting_hard = VotingClassifier(estimators=[('lr', logreg_X), ('rfc', rfc_X), ('svm', svm_X), ('dtc', dtc_X), ('etc', etc_X)], voting='hard')
voting_soft = VotingClassifier(estimators=[('lr', logreg_X), ('rfc', rfc_X), ('svm', svm_X), ('dtc', dtc_X), ('etc', etc_X)], voting='soft')

# Step 16
pipeline1_hyeri = Pipeline([('scaler', transformer_hyeri), ('classifier', RandomForestClassifier())])
pipeline2_hyeri = Pipeline([('scaler', transformer_hyeri), ('dtc', dtc_X)])

# Step 17
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
crossvalidation = KFold(n_splits=10, shuffle= True, random_state=14)

cv_results_pipeline1 = cross_val_score(pipeline1_hyeri, X, y, cv=crossvalidation, n_jobs=1)
cv_results_pipeline2 = cross_val_score(pipeline2_hyeri, X, y, cv=crossvalidation, n_jobs=1)

# Step 18
print("Mean score evaluation for Pipeline 1:", cv_results_pipeline1.mean())
print("Mean score evaluation for Pipeline 2:", cv_results_pipeline2.mean())

if cv_results_pipeline1.mean() > cv_results_pipeline2.mean():
    print("Pipeline 1 performed better")
else:
    print("Pipeline 2 performed better")

# Step 19
pipelines = [pipeline1_hyeri, pipeline2_hyeri]
pipeline_names = ["Pipeline 1", "Pipeline 2"]

for pipe in range(len(pipelines)):
    pipelines[pipe].fit(X_train_hyeri, y_train_hyeri)
    y_pred = pipelines[pipe].predict(X_test_hyeri)
    print(pipeline_names[pipe])
    print("Confusion Matrix:\n", confusion_matrix(y_test_hyeri, y_pred))
    print("Precision Score:", precision_score(y_test_hyeri, y_pred))
    print("Recall Score:", recall_score(y_test_hyeri, y_pred))
    print("F1 Score:", f1_score(y_test_hyeri, y_pred))
    print("Accuracy Score:", accuracy_score(y_test_hyeri, y_pred))

from sklearn.model_selection import RandomizedSearchCV


# define parameter distributions for randomized grid search
param_dist_n_estimators = {'classifier__n_estimators': range(10, 3001, 20)}
param_dist_max_depth = {'classifier__max_depth': range(1, 1001, 2)}

# define randomized grid search object
random_search = RandomizedSearchCV(
    estimator=pipeline1_hyeri,
    param_distributions=[param_dist_n_estimators, param_dist_max_depth],
    n_iter=50,
    cv=5,
    n_jobs=-1,
    random_state=42
)

# fit randomized grid search object to training data
random_search.fit(X_train_hyeri, y_train_hyeri)

# print best parameters and accuracy score
print('Best parameters:', random_search.best_params_)
print('Best accuracy:', random_search.best_score_)

# use best estimator to predict test data
y_pred = random_search.best_estimator_.predict(X_test_hyeri)

# print precision, recall, and accuracy scores
print('Precision:', precision_score(y_test_hyeri, y_pred))
print('Recall:', recall_score(y_test_hyeri, y_pred))
print('Accuracy:', accuracy_score(y_test_hyeri, y_pred))